{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36faae73-6397-476a-a740-fafd9f027a16",
   "metadata": {},
   "source": [
    "## Run an interactive chatbot locally using Langchain\n",
    "\n",
    "This tutorial is heavily inspired by the online course [LangChain Chat with your data](https://learn.deeplearning.ai/langchain-chat-with-your-data/lesson/1/introduction) at Deeplearning.ai.\n",
    "However I added a lot of my learnings and investigations for clarity and better understanding.\n",
    "\n",
    "We will be using a technique called Retrieval Augmented Generation.\n",
    "In retrieval augmented generation (RAG), an LLM retrieves contextual documents from an external dataset as part of its execution. \n",
    "\r\n",
    "This is useful if we want to ask question about specific documents (e.g., our PDFs, a set of videos, etc)\n",
    "\n",
    "To achieve that, we will be heavily relying on Langchain and many of its libraries.\n",
    "Last but not least all models and generated vectors are stored and run locally.\n",
    "\n",
    "**Disclaimer**: Despite the fact that all models and files remain local in this tutorial, it is not guaranteed and the author has no responsibility in case of data leaks using this code. Avoid using sensitive or classified information as part of this tutorial.\n",
    "\n",
    "### Dependencies\n",
    "We will be installing dependencies as we go so no worries on that one..\n",
    "This notebook was fully tested under WSL2. If you are using other platforms your mileadge may vary..\n",
    "Please keep in mind that this tutorial aims to load the models on the GPU for higher performance, but it is easily adaptable to run on the CPU instead "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f57dfd2-b8ae-4072-ae5d-24f22ecc0232",
   "metadata": {},
   "source": [
    "![RAG.jpg](./assets/RAG.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ecd815-0cb3-4f9d-ba5d-ff3fff2a2d87",
   "metadata": {},
   "source": [
    "## Document Loading\n",
    "\n",
    "The ffmpeg library might be needed for the Speech-to-Text models to run. Install it in the host system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba87d46-cee2-42d9-af3c-30e3a16f118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain torch\n",
    "#! apt install ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6595bb11-4945-488b-b647-83d4633a28b8",
   "metadata": {},
   "source": [
    "Basic imports and a helper function to show retrieved documents cleanly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4980689f-7d94-4d87-98fa-94258876ffbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content +\"\\n\" + str(d.metadata) for i, d in enumerate(docs)]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc60a7d-2d47-4d02-b363-269dcd58acd7",
   "metadata": {},
   "source": [
    "### Loading PDFs\n",
    "The first file we load is the Technical Summary of the IPCC Report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac81874-f532-41b7-81b8-1af95b9fc90d",
   "metadata": {},
   "source": [
    "Install dependency to load pdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaad7a0-1518-4d38-a194-0015a424340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pypdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b27fefa-d497-4984-95d7-19d62f700176",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [] # Will be used to store all our loaded documents\n",
    "\n",
    "# PDFs\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"docs/pdfs/IPCC_AR6_WGII_TechnicalSummary.pdf\")\n",
    "pages = loader.load()\n",
    "docs.extend(pages)\n",
    "print(len(docs))\n",
    "print(pages[0].page_content[0:500])\n",
    "print(pages[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b5b62f-f79d-4986-9fa2-6f163d216c68",
   "metadata": {},
   "source": [
    "### Load Youtube video transcripts\n",
    "\n",
    "In the below section we are going to show how to download and trandscribe a Youtube video using OpenAI's Whisper-Medium model.\n",
    "The first time you execute this it will download the model on your system. Transcription will run on your GPU but you can change to CPU if you want (for compatibility reasons). Keep in mind it will take a few minutes. The transcription will be then loaded into the docs.\n",
    "\n",
    "**Notice**: Youtube is changing its code very ofter, so the library we are using here (yt_dlp) might not work in the future. Please always make sure you have the latest version installed. If you run into an error, it is safe to continue with the tutorial without this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f67535-90e4-4a93-985e-38fa1e59079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install yt_dlp\n",
    "! pip install pydub\n",
    "! pip install transformers\n",
    "! pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b64872c-dbc5-44ad-b4d7-0af4dffd8b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Youtube\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers.audio import (\n",
    "    OpenAIWhisperParserLocal,\n",
    ")\n",
    "from langchain.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader\n",
    "\n",
    "url=\"https://www.youtube.com/watch?v=aywZrzNaKjs\"\n",
    "save_dir=\"docs/youtube/\"\n",
    "loader = GenericLoader(\n",
    "    YoutubeAudioLoader([url],save_dir),\n",
    "    OpenAIWhisperParserLocal(device=\"gpu\",lang_model=\"openai/whisper-medium\")\n",
    ")\n",
    "pages = loader.load()\n",
    "docs.extend(pages)\n",
    "print(len(docs))\n",
    "print(pages[0].page_content[0:500])\n",
    "print(pages[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3178284-bec2-4811-a65c-19db365e6b1b",
   "metadata": {},
   "source": [
    "### Loading URLs\n",
    "\n",
    "In the below section we are showing how to load URLs. As before, the loaded pages are added to the docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6332fb-fa03-46c4-bda4-6cab1d7889ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://python.langchain.com/docs/expression_language/get_started\")\n",
    "pages = loader.load()\n",
    "docs.extend(pages)\n",
    "print(len(docs))\n",
    "print(pages[0].page_content[0:100])\n",
    "print(pages[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc10e8e-f7dd-4ea1-a2e9-2fdd21d4bfbb",
   "metadata": {},
   "source": [
    "## Document Splitting\n",
    "\n",
    "In this section we are going to show how to split the documents in chunks that we can use for creating Embeddings later. You can see here that we are using a chunk_size of 1500 and we use an overlap of 150. The overlap helps in the continuity of the chunks and the context later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a68caa7-3add-45d1-8ba0-cc05389603cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 3,\n",
    "    chunk_overlap = 1\n",
    ")\n",
    "\n",
    "text1 = 'abcdefghijklmnopqrstuvwxyz'\n",
    "splits = text_splitter.split_text(text1)\n",
    "print(splits)\n",
    "\n",
    "# Our Doc Splitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "print(len(splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e50214-4d8b-4751-9965-c2a2255df420",
   "metadata": {},
   "source": [
    "## Creating Vector scores and Embeddings\n",
    "\n",
    "Let's take our splits and embed them. Embedding is the process of creating vectors using deep learning. An \"embedding\" is the output of this process â€” in other words, the vector that is created by a deep learning model for the purpose of similarity searches by that model.\n",
    "\n",
    "We are going to use the BGE-small model. You can try other models as well if you want to experiment.\n",
    "The first time you run this command it will download the model from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07613c4-0fcc-4e30-94c0-cb480393c7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c86b362-4474-4752-a8cf-477a2f8f74c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(\n",
    "                    model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "                    cache_folder=\"./models/\",\n",
    "                )\n",
    "\n",
    "embedding1 = embedding.embed_query(\"i like dogs\")\n",
    "print(embedding1[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5f0d60-49a6-4cf7-99bf-013a33351c6e",
   "metadata": {},
   "source": [
    "Next we will store those as vectors using Chromadb. The database is stored in a local folder called 'db'. At every execution, we clean the library and start from an empty one.\n",
    "\n",
    "**Notice**: In case this step fails in creating the db or importing chromadb you might need to restart the notebook kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af07a4e-77c4-494a-a71a-063a1068d383",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4044c2f0-8db0-42d9-93c9-894252c92bf4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "persist_directory = 'db'\n",
    "\n",
    "import shutil\n",
    "# remove old database files if any\n",
    "try:\n",
    "  shutil.rmtree(persist_directory)\n",
    "except Exception as e:\n",
    "  print(e)\n",
    "finally:\n",
    "    os.makedirs(persist_directory)\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "vectordb.persist()\n",
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a72b72e-54b8-42cd-a6a6-2f897222c786",
   "metadata": {},
   "source": [
    "### Retrieval Techniques\n",
    "\n",
    "In this step we are going to use some retrieval techniques to find similar or relevant information based on a question. There are many types of searches. We are going to see 3 of them in this example.\n",
    "\n",
    "#### Similarity search\n",
    "\n",
    "Similarity search will return... similar vectors to your question! We configured it here to return 3 results. More information can be found [in Langchain's documentation](https://python.langchain.com/docs/integrations/vectorstores/chroma).\n",
    "\n",
    "#### Max Marginal Relevance search\n",
    "\n",
    "This type of search behaves similar to the Similarity search but it filters down the results to maximize the \"new information\" of the overall retrieved documents. In this example we are fetching 4 documents and we are choosing the 3 that are the most \"irrelevant\" to each other, thus avoiding getting duplicate information that might be stored in different places of the ingested files/sources.\n",
    "\n",
    "#### Similarity Score search\n",
    "\n",
    "Similar to the similarity search but also returns the scores of the documents (lower is better). By default ChromaDb uses [L2 Distance](https://docs.trychroma.com/usage-guide#changing-the-distance-function) to score the documents. When defining a retriever we can also add the \"score_threshold\" we want in the \"search_kwags\". In our case we set it to 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f7dfac-bc06-493a-843f-ce1320b05a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Why should I use Langchain?\"\n",
    "\n",
    "# Test similarity search\n",
    "print(\"\\nSIMILARITY SEARCH\")\n",
    "docs_sim = vectordb.similarity_search(question,k=3)\n",
    "print(len(docs_sim))\n",
    "pretty_print_docs(docs_sim)\n",
    "\n",
    "question = \"What are some major risks highlighted in the IPCC report?\"\n",
    "\n",
    "# Test similarity search\n",
    "print(\"\\nSIMILARITY SEARCH\")\n",
    "docs_sim = vectordb.similarity_search(question,k=3)\n",
    "print(len(docs_sim))\n",
    "pretty_print_docs(docs_sim)\n",
    "\n",
    "# Test retrieval via mmr\n",
    "print(\"\\nMMR SEARCH\")\n",
    "docs_mmr = vectordb.max_marginal_relevance_search(question, k=3, fetch_k=4)\n",
    "print(len(docs_mmr))\n",
    "print(docs_mmr)\n",
    "pretty_print_docs(docs_mmr)\n",
    "\n",
    "# Test retrieval via similarity score\n",
    "print(\"\\nSIMILARITY SCORE SEARCH\")\n",
    "docs_sim_score_tuple = vectordb.similarity_search_with_score(question, k=3)\n",
    "# print(docs_sim_score_tuple)\n",
    "scores = [d[1] for d in docs_sim_score_tuple]\n",
    "print(scores) # The lower the better\n",
    "docs_sim_score = [d[0] for d in docs_sim_score_tuple]\n",
    "pretty_print_docs(docs_sim_score)\n",
    "\n",
    "# Create and Test retrievers\n",
    "\n",
    "print(\"\\nMMR SEARCH\")\n",
    "retriever_mmr = vectordb.as_retriever(search_type = \"mmr\", search_kwargs={\"k\":3, \"fetch_k\":4})\n",
    "docs = retriever_mmr.get_relevant_documents(question)\n",
    "pretty_print_docs(docs)\n",
    "assert(docs == docs_mmr)\n",
    "\n",
    "print(\"\\nSIMILARITY SCORE SEARCH\")\n",
    "retriever_score = vectordb.as_retriever(search_type = \"similarity_score_threshold\", search_kwargs={\"k\":3, \"score_threshold\": 0.5})\n",
    "docs = retriever_score.get_relevant_documents(question)\n",
    "pretty_print_docs(docs)\n",
    "assert(docs == docs_sim_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0888d26e-101c-45cc-bec6-82e0b1453663",
   "metadata": {},
   "source": [
    "#### Retrieval via LLMs (compression)\n",
    "\n",
    "In this section we will see one more way to retrieve documents from our Vector database and compare it with the MMR search of the section before and the Similarity score. In order to be able to pull more context our of the ingested documents, we can assign an LLM to \"summarize\"/\"compress\" the results. We are going to use the open source Mistral Instruct model as a compressor retriever, and we will use this until the end of the tutorial.\n",
    "\n",
    "We set the context for the model we are using to the maximum (32786) and temperature to zero for repeatable and accurate compression.\n",
    "Note that the compression_retriever requires a base_retriever and we are using the retriever_score (Similarity Score search) we created earlier. Also, since we are running an LLM invokation for each retrieved document, this retriever makes chating with the LLM later on quite much longer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a5f065-c629-44fd-979c-d6d2729d42db",
   "metadata": {},
   "source": [
    "With this command you can install the cuda-enabled llama-cpp-python library. This command might defer depending on your system (Windows, Linux, MacOS). This one is needed to run llama.cpp on the GPU on Linux/WSL. More information in the [llama.cpp documentation](https://llama-cpp-python.readthedocs.io/en/latest/).\n",
    "\n",
    "**Notice**: In case you have issues installing llama-cpp-python refer to [this PR](https://github.com/oobabooga/text-generation-webui/issues/1534)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee0160a-016b-4b98-b8ba-4976daa40d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! CMAKE_ARGS='-DLLAMA_CUBLAS=on' pip install --force-reinstall --no-cache-dir llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a99e44-8e6f-4037-a28f-7bf17e256fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5358515b-84a5-4422-b9c2-fcf78c0fd908",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "from os.path import expanduser\n",
    "import wget\n",
    "\n",
    "# Compression retriever via llm\n",
    "\n",
    "model_url = \"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf?download=true\"\n",
    "model_path = \"models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\n",
    "if not os.path.isfile(model_path):\n",
    "    print(\"Downloading model\")\n",
    "    wget.download(model_url, model_path)\n",
    "else:\n",
    "    print(\"Model already downloaded\")\n",
    "\n",
    "# Initialize a model for sequence-to-sequence tasks using the specified pretrained model\n",
    "compress_llm = LlamaCpp(\n",
    "    model_path=\"./models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n",
    "    streaming=False,\n",
    "    n_ctx=32768,\n",
    "    temperature=0,\n",
    ")\n",
    "compressor = LLMChainExtractor.from_llm(compress_llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever_score\n",
    ")\n",
    "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4610a7-dc91-4036-b015-41e51c297b40",
   "metadata": {},
   "source": [
    "### Q&A\n",
    "\n",
    "In this section we will start asking questions to our model while providing it with context. We will also set up a prompt template that is instructing our model with what we expect. In the last part we will use a Chat chain to maintain the chat history as part of the context of the model.\n",
    "\n",
    "#### Loading of the Llama 2 Chat Model\n",
    "\n",
    "We are going to use the same Mistral 7b instruct LLAma CPP model and we will augment Llama-2 LLMs with the Llama2Chat wrapper to support the [Llama-2 chat prompt format](https://huggingface.co/blog/llama2#how-to-prompt-llama-2). More examples in the [Langchain Docs](https://python.langchain.com/docs/integrations/chat/llama2_chat)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2969e77c-7ddd-488e-b13e-65769915e197",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain_experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd292bc-15fc-452d-a685-8b14e2b18f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a chat model to retrieve an answer to a question\n",
    "model_name = \"./models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\n",
    "model_path = expanduser(model_name)\n",
    "\n",
    "llm_model = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    streaming=False,\n",
    "    n_ctx=4096,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "from langchain_experimental.chat_models import Llama2Chat\n",
    "\n",
    "llm = Llama2Chat(llm=llm_model)\n",
    "llm.invoke(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d891bdd-d542-4f9b-94c9-611f83e01fb1",
   "metadata": {},
   "source": [
    "#### Single answer retrieval\n",
    "\n",
    "In this step we are showing how to retrieve an answer without chat history. We are using a prompt template that specifies where the {context} and {question} should go. Please note that we are using the compression_retriever we defined earlier as the chain retriever. Feel free to experiment with others too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8a1433-1fb9-450e-bb5d-82851f2663c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template_no_hist = \"\"\"[INST] <<SYS>>\n",
    "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "Context:\n",
    "{context}\n",
    "<</SYS>> \n",
    "Question:\n",
    "{question} [/INST] \n",
    "\"\"\"\n",
    "A_CHAIN_PROMPT = PromptTemplate.from_template(template_no_hist)\n",
    "\n",
    "a_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=compression_retriever,\n",
    "    return_source_documents=True,\n",
    "    verbose=True,\n",
    "    chain_type_kwargs={\"prompt\": A_CHAIN_PROMPT}\n",
    ")\n",
    "\n",
    "result = a_chain({\"query\": question})\n",
    "print(result[\"result\"])\n",
    "pretty_print_docs(result[\"source_documents\"])\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5800df32-a62b-47c7-8f82-1fbb921119c0",
   "metadata": {},
   "source": [
    "#### Creating the Q&A chat prompt template\n",
    "\n",
    "In this step we are preparing a prompt template compatible with Llama 2 format that supports chat history. Also we are specifying where the {context}, the {chat_history} and the {question} should go. This template is going to be used every time we are asking something to the Chat LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3a74d5-1ef2-4bc7-b058-fdd5ad218159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Build prompt\n",
    "template = \"\"\"[INST] <<SYS>>\n",
    "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "Context:\n",
    "{context}\n",
    "<</SYS>> \n",
    "Chat History: \n",
    "{chat_history}\n",
    "Question: \n",
    "{question} [/INST] \n",
    "\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "print(QA_CHAIN_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba48de90-1601-4f16-95cc-17455ecbc3f7",
   "metadata": {},
   "source": [
    "#### Q&A with chat history\n",
    "\n",
    "For this section we will use the ConversationalRetrievalChain class to create a chain and interact with our Chat model. I highly suggest reading thourgh the [Langchain docs and API](https://api.python.langchain.com/en/latest/chains/langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain.html#langchain-chains-conversational-retrieval-base-conversationalretrievalchain) since this class has many options.\n",
    "\n",
    "This chain is using a condense step to summarize the chat history and pass it on to the Chat model to answer follow up questions. We show here how you could use you own, however Langchain has a prompt like this by default which normally should produce good results. Check [this PR](https://github.com/langchain-ai/langchain/issues/4076) for more explanation on the topic.\n",
    "\n",
    "For the chat history, we are keeping track of every Q&A step and pass it in the chaing. Alternatively you can use the ConversationBufferMemory class and pass it to the chain (here commented out).\n",
    "\n",
    "Look at the logs for more information on what is happening during this chain execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0c051e-0c70-4b85-8b1d-221988ac81d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "#from langchain.memory import ConversationBufferMemory\n",
    "#memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key='question', output_key='answer', return_messages=True)\n",
    "chat_history = []\n",
    "\n",
    "condense_question_template = \"\"\"\n",
    "    [INST] <<SYS>>\n",
    "    Return text in the original language of the follow up question.\n",
    "    If the follow up question does not need context, return the exact same text back.\n",
    "    Rephrase the follow up question based on the chat history only if it needs context.\n",
    "    <</SYS>> \n",
    "    Chat History: {chat_history}\n",
    "    Follow Up question: {question}\n",
    "    Standalone question: [/INST] \n",
    "\"\"\"\n",
    "condense_question_prompt = PromptTemplate.from_template(condense_question_template)\n",
    "\n",
    "conv_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=compression_retriever,\n",
    "    # memory=memory,\n",
    "    return_generated_question=True,\n",
    "    return_source_documents=True,\n",
    "    combine_docs_chain_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "    verbose=True,\n",
    "    # response_if_no_docs_found=\"No context found!\",\n",
    "    rephrase_question=False, # Not sure if this does anything...\n",
    "    # condense_question_prompt=condense_question_prompt\n",
    ")\n",
    "result = conv_chain({\"question\": question, \"chat_history\": chat_history})\n",
    "print(result[\"answer\"])\n",
    "pretty_print_docs(result[\"source_documents\"])\n",
    "print(result)\n",
    "chat_history.extend([(question, result[\"answer\"])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183197a3-6120-4d2e-92a2-092ebdc6ea5f",
   "metadata": {},
   "source": [
    "#### Asking Follow-up questions\n",
    "\n",
    "Here we are asking 2 follow up questions to the Chat LLM to test the chat history and the context. Pay attention to the steps in the verbose logs. Also, at the end, we are showing how to clear the chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117a6a1f-d16f-4e7a-b212-de8dec5e52be",
   "metadata": {},
   "outputs": [],
   "source": [
    "fu_question = \"Can you re-write your response so a 10-year old kid can understand?\" \n",
    "result = conv_chain({\"question\": fu_question, \"chat_history\": chat_history})\n",
    "print(result[\"answer\"])\n",
    "pretty_print_docs(result[\"source_documents\"])\n",
    "print(result)\n",
    "chat_history.extend([(fu_question, result[\"answer\"])]) # Save Q&A in chat history\n",
    "\n",
    "fu_question = \"What was my previous question?\" \n",
    "result = conv_chain({\"question\": fu_question, \"chat_history\": chat_history})\n",
    "print(result[\"answer\"])\n",
    "pretty_print_docs(result[\"source_documents\"])\n",
    "print(result)\n",
    "chat_history = [] # Clears the chat history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
